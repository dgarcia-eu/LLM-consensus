---
title: "Language Understanding as a Constraint on Consensus Size in LLM Societies"
author: "David Garcia <br> *University of Konstanz <br> Barcelona Supercomputing Center <br>  Complexity Science Hub Vienna*"
date: "slides at dgarcia.eu/LLM-consensus"
output:
  xaringan::moon_reader:
    lib_dir: libs 
    css: [xaringan-themer.css, "libs/footer.css"]
    nature:
      beforeInit: ["libs/perc.js"]
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
---

```{r xaringan-themer, include=FALSE, warning=FALSE}
#This block contains the theme configuration for the CSS lab slides style
library(xaringanthemer)
library(showtext)
style_mono_accent(
  base_color = "#5c5c5c",
  text_font_size = "1.5rem",
  header_font_google = google_font("Arial"),
  text_font_google   = google_font("Arial", "300", "300i"),
  code_font_google   = google_font("Fira Mono")
)
```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
```



layout: true
<div class="my-footer"><span><a href="https://arxiv.org/abs/2312.06619"> Emergence of Scale-Free Networks in Social Interactions among Large Language Models. G. De Marzo, L. Pietronero, D. Garcia. https://arxiv.org/abs/2312.06619</a></span></div>

---

background-image: url(figures/AboutUS.svg)
background-size: 98%

---

## On Digital Traces and Agent-Based Models

.pull-left[
![:scale 100%](figures/CollectiveEmotionsParis.svg)
Human behavior from digital traces:  
e.g. Collective emotions on Twitter
]

.pull-right[
![:scale 97%](figures/ABM.svg)

Explaining collective behavior:  
e.g. ABM of collective emotions  
**Problem: calibrating micro-dynamics of agents**
]

---

## Social Simulation with Generative Agents

.pull-left[![:scale 100%](figures/Simulacra.png)
<div class="ref"><span><a href="https://arxiv.org/pdf/2304.03442"> Generative Agents: Interactive Simulacra of Human Behavior. S. Park et al (2023)</a></span></div>]

.pull-right[.center[![:scale 100%](figures/Bail.png)]
<div class="ref"><span><a href="https://arxiv.org/abs/2312.06619"> Simulating Social Media Using Large Language Models to Evaluate Alternative News Feed Algorithms. P. TÃ¶rnberg et al (2023) </a></span></div>]

---
# The WHAT-IF HORIZON project

.center[![:scale 50%](figures/WHAT-IF.png)]

WHat-if: Advanced Simulations for Testing the Effect of the Information Environment on the Functioning of Democracy 
---

## Generative agents in an online social network
.center[![:scale 75%](figures/LLMGrowth.png)]

---

# Network structures and interface design

.center[![:scale 100%](figures/LLMnet2.png)]

---

# Microdynamics of link creation
.center[![:scale 100%](figures/LLMnet3.png)]

- Generative agents with Large Language Models create scale-free synthetic online social networks  
- <div class="ref"><span><a href="https://arxiv.org/abs/2312.06619"> Emergence of Scale-Free Networks in Social Interactions among Large Language Models. G. De Marzo, L. Pietronero, D. Garcia. https://arxiv.org/abs/2312.06619</a></span></div>

---


layout: true
<div class="my-footer"><span><a href="https://arxiv.org/abs/2409.02822"> Language Understanding as a Constraint on Consensus Size in LLM Societies. G de Marzo, C. Castellano, D. Garcia. Arxiv preprint (2024)</a></span></div> 

---
# LLMs Within Society

.center[![:scale 88%](figures/AIeverywhere.png)]
- AI "chiefs of staff" promise to interact with each other in our behalf
- Coordination and competition (reservations, negotiations, applications)
- **Could norms emerge, for example rules to be more efficient?** 
- **Could they have systemic risks, like flash crashes?**

---

# LLMs as Social Brains
.pull-left[![:scale 100%](figures/DunbarQuestion.svg)]
.pull-right[

- Group formation and sustainability: size depends on cognitive ability


- Memory of identity to predict behavior and cooperation


- Essential coordination on arbitrary norms without ground truth
  - E.g. pack moves left or right


- Language as a tool for humans to make larger groups:
  - Dunbar's number (150-250)


]
---

# Opinion Dynamics in LLM Agents
.pull-left[.center[![:scale 75%](figures/OP.png)]]
.pull-right[
- Simulation of a tight group of N interacting agents
- Agents start with a random opinion of two options
- Each iteration, they see the opinions of all others (prompt)
- They respond to the question of their opinion
- Opinion labels need to be random and shuffled to avoid token biases
- Consensus is achieved if all have the same opinion
]

---

# LLM-Dependent Consensus Formation
.center[![:scale 75%](figures/Consensus.png)]
Some LLMs can reach consensus for completely arbitrary decisions (50 agents)

---

# Understanding LLM Opinion Dynamics

.center[![:scale 94%](figures/Sigma.png)]
Agent opinion changes follow an S-function parametrized by a majority force $\beta$
---

# Majority Force Factors
.center[![:scale 95%](figures/plot2.png)]
- Majority force is higher for models with higher language understanding capabilities (MMLU benchmark)
- Majority force decreases for larger group sizes

---

# Ising model warning

</br>

> <font size="45">
No please, not another Ising model
</font> 

</br>

*<div style="text-align: right"> 
David Garcia, Vienna, 2017-2020, every week
</div>*



---
## Critical Group Size and Consensus Time $T_c$
.pull-right[![:scale 100%](figures/CW.png)]

- Analysis of critical group size $N_c$


- $N>N_c$: time to consensus grows expontentially with $N$


- Above critical size, consensus is unfeasible and happens only by chance


- $T_c$ can be calculated from $\beta$ as in the Curie-Weiss model (i.e. time to magnetization as a function of inverse temperature)

- $N_c$ can be derived from  $\beta$ as the point of phase transition of $T_c$ ( $\beta_c=1$ )


---

# Group Size and Language Understanding
.pull-right[![:scale 90%](figures/Scaling.png)]

- Analysis of majority force and exhaustive simulations to measure **critical consensus size**

- Exponential function of MMLU benchmark

- Humans close to the line

- GPT4 and Claude 3.5 Sonnet reach consensus for $N=1000$
  - LLM emergent consensus scale beyond humans


---

# Summary

- Generative agents can create realistic social network degree distributions
- LLM consensus scale predicted by language understanding capabilities
- LLMs can reach emergent consensus at scales beyond humans
- **Opportunity: decision-making or coordination?**
- **Risk: undesired synchronization like a flash crash?**
- **Future: Social simulation with LLMs **

<a href="https://arxiv.org/abs/2312.06619"> Emergence of Scale-Free Networks in Social Interactions among Large Language Models. G. De Marzo, L. Pietronero, D. Garcia. Arxiv (2013) </a>

<a href="https://arxiv.org/abs/2409.02822"> Language Understanding as a Constraint on Consensus Size in LLM Societies. G de Marzo, C. Castellano, D. Garcia. Arxiv (2014) </a>

.center[**More at: [www.dgarcia.eu](https://dgarcia.eu)**]
.center[**[Bluesky: @dgarcia.eu](https://bsky.app/profile/dgarcia.bsky.social)**]

